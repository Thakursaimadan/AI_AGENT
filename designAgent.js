import { StateGraph, MessagesAnnotation } from "@langchain/langgraph";
import { AzureChatOpenAI } from "@langchain/openai";
import dotenv from "dotenv";
dotenv.config();

// âœ… Enhanced DesignAgent Prompt with Full Schema and Mapping Logic
export const designPrompt = `
You are DesignAgent. You handle design creation and modification by mapping natural language to a nested schema.

## DATABASE SCHEMA OVERVIEW:
Design includes:
- HeaderDesign: Layout, banner_image_url, social_icon_style
- ColorPalate: primary, secondary, tertiary
- Appearance: background, GradientProp(GradientType, GradientColorType), libraryID, filter, AnimationName
- Blocks: page, LinkBlock, CardBlock, desktopBackground
- Design aspects: CardDesign, ButtonDesign (style, Radius)
- Text: titles, subtitles

## OPERATION TYPES:
- CREATE, READ, UPDATE, DELETE

## FIELD MAPPING:
- "header image" â†’ HeaderDesign.banner_image_url
- "primary color" â†’ ColorPalate.primary
- "background type" â†’ Appearance.background
- "gradient style" â†’ Appearance.GradientProp.GradientType
- "gradient colors" â†’ Appearance.GradientProp.GradientColorType
- "animation" â†’ Appearance.AnimationName
- "card style" â†’ CardDesign.style
- "button radius" â†’ ButtonDesign.Radius
- "title text" â†’ Text.titles

## REQUIRED INFO:
- clientId: always required
- target field(s) and value(s): required for update
- deletion intent must be explicit

## RESPONSE FORMAT:
- Summary of action
- Mapped fields
- Execution result (success/failure)
- Next step if needed

If info is missing, STOP and ask ClarifierAgent for help.
`;

// ğŸ§  LLM with updated designPrompt
const designLLM = new AzureChatOpenAI({
	azureOpenAIEndpoint: process.env.AZURE_OPENAI_ENDPOINT,
	azureOpenAIApiKey: process.env.AZURE_OPENAI_API_KEY,
	azureOpenAIApiDeploymentName: "gpt-4o",
	azureOpenAIApiVersion: "2025-01-01-preview",
	temperature: 0,
});

// ğŸ” Core design LLM call node
const callDesignModel = async (state) => {
	console.log("ğŸ¨ Calling Design Model with state:", state);

	const lastHumanMessage = state.messages
		.filter((m) => m._getType() === "human")
		.pop();

	if (lastHumanMessage) {
		console.log("ğŸ§  Last human message:", lastHumanMessage.content);
	}

	const response = await designLLM.invoke(state.messages);
	console.log("ğŸ¯ Design Model response:", response.content);

	if (response.tool_calls) {
		response.tool_calls.forEach((toolCall, index) => {
			console.log(`ğŸ”§ Tool call ${index + 1}:`, {
				name: toolCall.name,
				args: toolCall.args,
			});
		});
	}

	return { messages: [response] };
};

// ğŸ› ï¸ Tool execution node (placeholder to be filled in)
const callDesignTools = async (state) => {
	console.log("ğŸ› ï¸ Executing Design Tools with state:", state);
	// TODO: implement updateDesign, insertDesign, deleteDesign handlers
	return state;
};

// ğŸ¤– Conditional branching
const shouldContinueDesign = (state) => {
	const lastMessage = state.messages[state.messages.length - 1];
	const type = lastMessage.type || lastMessage._getType();
	console.log("ğŸ¤” Should continue? Last message type:", type);

	if (lastMessage.tool_calls && lastMessage.tool_calls.length > 0) {
		console.log("â¡ï¸ Detected tool calls â€“ going to tools");
		return "tools";
	}

	console.log("âœ… No tools needed â€“ ending");
	return "__end__";
};

// ğŸš€ Compile the full agent graph
export const designAgent = new StateGraph(MessagesAnnotation)
	.addNode("llmCall", callDesignModel)
	.addNode("tools", callDesignTools)
	.addEdge("__start__", "llmCall")
	.addConditionalEdges("llmCall", shouldContinueDesign)
	.addEdge("tools", "llmCall")
	.compile();
